{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### image feature map extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "option = 'in792sx'\n",
    "feature_num = \"1\"\n",
    "\n",
    "## data directory\n",
    "data_root_path = 'E:\\workspace\\PROJECT\\doosan\\doosan_sem-1\\in792sx'    \n",
    "image_path = 'E:\\workspace\\PROJECT\\doosan\\doosan_sem-1\\in792sx\\images'\n",
    "\n",
    "model_path = 'E:\\workspace\\PROJECT\\doosan\\doosan_sem-1\\IN792sx_gamma_best.pt'\n",
    "\n",
    "with open(os.path.join(data_root_path, 'images.txt')) as f:\n",
    "    lines = f.readlines()\n",
    "data_list = [line.rstrip('\\n') for line in lines]\n",
    "        \n",
    "model = smp.DeepLabV3('resnet34', encoder_depth=4, encoder_weights=None, in_channels=1,decoder_channels=32)\n",
    "model.load_state_dict(torch.load(os.path.join(model_path)))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_name = []\n",
    "feature_map=[]\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "for data in data_list:\n",
    "    data = data.split(' ')[0]\n",
    "    img = Image.open(os.path.join(image_path, data))\n",
    "    img = transform(img).cuda().unsqueeze(0)\n",
    "\n",
    "    image_name.append(data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, features, decoder_output = model(img)\n",
    "        \n",
    "        ## Extract and save feature maps from encoder(->features) and decoder(->decoder_output)\n",
    "        if feature_num == \"1\":\n",
    "            feature_map.append(features[0])\n",
    "            feature_name = \"f1\"\n",
    "        elif feature_num == \"2\":\n",
    "            feature_map.append(features[1])\n",
    "            feature_name = \"f2\"\n",
    "        elif feature_num == \"3\":\n",
    "            feature_map.append(features[2])\n",
    "            feature_name = \"f3\"\n",
    "        elif feature_num == \"4\":\n",
    "            feature_map.append(features[3])\n",
    "            feature_name = \"f4\"\n",
    "        elif feature_num == \"5\":\n",
    "            feature_map.append(features[4])\n",
    "            feature_name = \"f5\"\n",
    "        elif feature_num == \"6\":\n",
    "            feature_map.append(decoder_output)\n",
    "            feature_name = \"decoder_output\"\n",
    "\n",
    "data={\n",
    "        'image_name' : image_name,\n",
    "        'feature_map':feature_map,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['210324-409-1_m001_r1.png',\n",
       " '210324-409-1_m002_r1.png',\n",
       " '210324-409-1_m003_r1.png',\n",
       " '210324-409-1_m004_r1.png',\n",
       " '210324-409-1_m005_r1.png',\n",
       " '210324-409-1_m006_r1.png',\n",
       " '210324-409-1_m007_r1.png',\n",
       " '210324-409-1_m008_r1.png',\n",
       " '210324-409-1_m009_r1.png',\n",
       " '210324-409-1_m010_r1.png',\n",
       " '210324-409-1_m011_r1.png',\n",
       " '210324-409-1_m012_r1.png']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4549, 0.4471, 0.4549,  ..., 0.5882, 0.6235, 0.6196],\n",
       "          [0.4549, 0.4471, 0.4549,  ..., 0.5725, 0.6157, 0.6275],\n",
       "          [0.4510, 0.4510, 0.4431,  ..., 0.5686, 0.6157, 0.6392],\n",
       "          ...,\n",
       "          [0.3961, 0.3961, 0.3843,  ..., 0.6000, 0.5922, 0.5804],\n",
       "          [0.3961, 0.3961, 0.3843,  ..., 0.5804, 0.5608, 0.5333],\n",
       "          [0.4000, 0.3882, 0.3804,  ..., 0.5647, 0.5373, 0.4980]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['feature_map'][0].shape)\n",
    "data['feature_map'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = r\"E:/workspace/PROJECT/doosan/doosan_sem-1/regression/data_all_features_add_image.csv\"\n",
    "dataset = pd.read_csv(data_path, encoding='UTF-8', sep=',') # integrated version of IN792sx, interrupt, cm939w data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "independent variable: (1)이미지 피쳐맵\n",
      "╒════╤══════╤════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│    │   id │ Name       │ image_feature_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ image_feature_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 │ image_feature_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          │ image_feature_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │ image_feature_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             │ image_feature_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            │\n",
      "╞════╪══════╪════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│  0 │  7_1 │ 7_1_1.png  │ [[0.12024862319231033, 0.15066924691200256, 0.15458212792873383, 0.12471770495176315, 0.0643448755145073, 0.09261339902877808, 0.1591368019580841, 0.07433082163333893, 0.14045806229114532, 0.1670282483100891, 0.0844307467341423, -0.022721990942955017, 0.10065209865570068, 0.13085989654064178, 0.18433454632759094, 0.09650138020515442, 0.17410188913345337, 0.12472474575042725, 0.0964793711900711, -0.022109761834144592, 0.13452516496181488, 0.07911118865013123, 0.12352844327688217, 0.13307176530361176, 0.10974333435297012, 0.03710899502038956, 0.049498990178108215, 0.092969611287117, 0.15435926616191864, 0.12668094038963318, 0.2694595456123352, 0.19392040371894836]]            │ [[0.11661333590745926, 0.13787168264389038, 0.26531174778938293, 0.058709949254989624, -0.1154618114233017, 0.12231864035129547, 0.01282491721212864, -0.030262377113103867, 0.4488532245159149, -0.2062523365020752, 0.16520075500011444, 0.015046004205942154, -0.12289723753929138, -0.3001203238964081, 0.1799519807100296, -0.017354167997837067, 0.18445400893688202, 0.14093822240829468, 0.18921330571174622, -0.09159677475690842, 0.1731228530406952, 0.16933287680149078, 0.1884642094373703, 0.049475669860839844, 0.11572278290987015, 0.0475083589553833, -0.12042291462421417, 0.12142027169466019, -0.09082725644111633, 0.021626096218824387, 0.285945862531662, -0.10304971784353256]]        │ [[-0.2253749519586563, -0.2675281763076782, -0.15324504673480988, 0.0655483528971672, -0.3664665222167969, -0.2843469977378845, -0.17808416485786438, 0.6387984752655029, 0.10603529214859009, -0.010500339791178703, 0.22728091478347778, -0.17787031829357147, -0.3808422088623047, -0.13348540663719177, 0.46260619163513184, -0.06167110800743103, 0.13171276450157166, 0.0472535565495491, -0.1956353634595871, 0.6841576099395752, 0.24080024659633636, -0.13247230648994446, 0.47745439410209656, -0.47436168789863586, -0.0761098861694336, -0.17149126529693604, 0.579797625541687, 0.13851183652877808, -0.11856287717819214, 0.3263085186481476, -0.11188282817602158, -0.26536333560943604]] │ [[-0.3594183325767517, 0.003142327070236206, 0.3205561935901642, 0.49289217591285706, 0.0707588717341423, -0.8432680368423462, 0.49540671706199646, -0.5465477108955383, -0.4768933355808258, -0.3370409607887268, -0.29170963168144226, 0.5054008364677429, -0.36097484827041626, -0.8050111532211304, -0.2756824493408203, 0.3202604651451111, 0.14011907577514648, -0.14632081985473633, 0.43329110741615295, -0.5075022578239441, -0.03961913287639618, -0.10615084320306778, 0.2426813542842865, 0.5407631993293762, 0.22553938627243042, 0.5216824412345886, -0.6572583317756653, 0.1792784333229065, -0.5252973437309265, 0.058667443692684174, 0.3921414911746979, -0.3662143647670746]]           │ [[0.05846637487411499, -0.09212616086006165, 0.09683459997177124, -0.25810378789901733, -0.024560943245887756, -0.16736923158168793, -0.059235043823719025, 0.061489641666412354, 0.45018357038497925, 0.605158805847168, -0.038356244564056396, 0.4129043221473694, -0.28915876150131226, -0.4063294529914856, -0.1416471004486084, 0.2069094330072403, 0.5000678300857544, -0.07259978353977203, 0.0805777758359909, -0.04180821776390076, 0.08783406019210815, -0.03362464904785156, 0.4221392273902893, 0.5289028882980347, -0.12473765015602112, 0.13825339078903198, 0.12637408077716827, -0.2853296995162964, -0.152788907289505, -0.056657448410987854, -0.054744355380535126, 0.373283326625824]]  │ [[-0.35045769810676575, -0.2591642439365387, -0.27381688356399536, -0.42889219522476196, 0.1832605004310608, 0.06073319911956787, -0.5392374992370605, -0.15392129123210907, -0.21164724230766296, -0.2173205316066742, 0.1906823217868805, 0.11917892098426819, -0.25712156295776367, -0.024965200573205948, -0.033583398908376694, -0.03641960769891739, 0.1312219351530075, -0.3420238494873047, -0.32230737805366516, 0.05236412584781647, -0.012140929698944092, -0.151460200548172, -0.28038302063941956, -0.06039341539144516, 0.20102432370185852, 0.18990623950958252, -0.035563379526138306, -0.06174202263355255, 0.11618658900260925, -0.275870144367218, 0.05932070314884186, -0.20948198437690735]]          │\n",
      "│  1 │  7_1 │ 7_1_10.png │ [[0.0669223964214325, 0.2487642467021942, 0.1901012808084488, 0.12121929973363876, 0.05478738993406296, 0.07040856778621674, 0.1435389369726181, 0.08689997345209122, 0.14670702815055847, 0.006935141980648041, 0.18057937920093536, 0.005118533968925476, 0.16716288030147552, 0.14843876659870148, 0.2150985151529312, 0.16703763604164124, 0.21703743934631348, 0.1462763398885727, 0.12320207059383392, 0.004192955791950226, 0.08522378653287888, 0.04418816417455673, 0.21500620245933533, 0.057236216962337494, 0.12545828521251678, 0.03425155580043793, 0.06529679894447327, 0.15543600916862488, 0.12073786556720734, 0.04870624095201492, 0.2755334973335266, 0.26108551025390625]]            │ [[0.1659088283777237, 0.29244664311408997, -0.11780859529972076, -0.31090348958969116, -0.3654329478740692, -0.023938346654176712, 0.039494194090366364, 0.07995253801345825, 0.22641178965568542, -0.10442695766687393, 0.05981176719069481, 0.07777344435453415, -0.2829931676387787, -0.3285899758338928, -0.018703393638134003, 0.22891247272491455, -0.12253125011920929, -0.06412331759929657, 0.07830055803060532, 0.26391085982322693, 0.2655452489852905, 0.3021794557571411, -0.206556037068367, 0.04437059164047241, 0.23801757395267487, 0.03793965280056, -0.034059666097164154, 0.08959852159023285, 0.27113446593284607, 0.08112884312868118, 0.054645273834466934, -0.10027413070201874]]       │ [[0.6055417060852051, 0.05654902756214142, 0.012176195159554482, -0.7583011388778687, -0.4900454580783844, -0.3478360176086426, -0.09632518142461777, 0.4077373147010803, -0.3656230866909027, 0.7391718029975891, 0.35691729187965393, 0.2045307606458664, -0.589231550693512, 0.029103582724928856, 0.14759114384651184, 0.04585440456867218, 0.23630352318286896, -0.2701977491378784, 0.1444021761417389, -0.07212189584970474, -0.5554837584495544, -0.42905837297439575, 0.04790172725915909, -0.41757169365882874, -0.13139726221561432, -0.6033583283424377, -0.3539509177207947, 0.2582395076751709, 0.17623202502727509, -0.14827565848827362, -0.44832056760787964, -0.04023762792348862]]    │ [[-0.030698813498020172, 0.15434464812278748, -0.5348121523857117, 0.8162883520126343, 0.783849835395813, -0.2747780978679657, -0.10606209933757782, 0.03653798997402191, -0.25498446822166443, -0.07937657088041306, -0.3876532018184662, 0.612197995185852, -0.021822676062583923, -0.8442681431770325, -0.02822652831673622, 0.08368072658777237, -0.37228667736053467, -0.7502124309539795, 0.03831374645233154, 0.11240207403898239, 0.5828762054443359, -0.28922683000564575, 0.5072873830795288, -0.16596302390098572, 0.29475095868110657, -0.23883482813835144, -0.16350410878658295, -0.1560741662979126, -0.5888199210166931, -0.2768480181694031, -0.01039654016494751, -0.23407712578773499]] │ [[0.10532476007938385, -0.15585653483867645, 0.688987135887146, -0.9344033598899841, 0.4593748450279236, -0.5135942697525024, 0.1056254655122757, 0.5594408512115479, -0.006847694516181946, 0.08611512184143066, -0.2526096701622009, 0.08682383596897125, 0.612142026424408, 0.1288815587759018, 0.5052974820137024, 0.5660015940666199, 0.08768518269062042, 0.22214370965957642, 0.22820645570755005, -0.21661560237407684, 0.5285657644271851, 0.031477347016334534, 0.5829162001609802, -0.023716747760772705, 0.08506971597671509, 0.24696967005729675, -0.3385721445083618, -0.48026496171951294, 0.07624118775129318, 0.035997241735458374, -0.3013131618499756, 0.7872039079666138]]              │ [[-0.009201321750879288, -0.26059597730636597, 0.11305846273899078, -0.5155569314956665, 0.12387791275978088, 0.22870904207229614, 0.04218413680791855, 0.19262248277664185, 0.15655606985092163, -0.20001757144927979, -0.16731925308704376, 0.12686648964881897, -0.07344597578048706, 0.049736782908439636, -0.06004003435373306, -0.1682809740304947, -0.041745543479919434, -0.38933560252189636, -0.25243937969207764, -0.44668465852737427, 0.11737789213657379, -0.2099076509475708, 0.03835219889879227, 0.12839065492153168, 0.34608304500579834, -0.23314860463142395, -0.08173927664756775, 0.3088412582874298, 0.2456212043762207, -0.3394591808319092, -0.22062043845653534, -0.19047659635543823]]          │\n",
      "│  2 │  7_1 │ 7_1_11.png │ [[0.12086768448352814, 0.23515920341014862, 0.2139362096786499, 0.13982968032360077, 0.05255644768476486, 0.056481968611478806, 0.1339728683233261, 0.11018373817205429, 0.13307508826255798, 0.03408756107091904, 0.0764448344707489, 0.07261285930871964, 0.04340200126171112, 0.14443683624267578, 0.1510465443134308, 0.06263074278831482, 0.22607934474945068, 0.10234315693378448, 0.11633061617612839, -0.048311203718185425, 0.1552709937095642, 0.047579020261764526, 0.22861866652965546, 0.14001989364624023, 0.16641966998577118, 0.008351325988769531, 0.11043664067983627, 0.03426242619752884, 0.11776769906282425, 0.1507573425769806, 0.2528606653213501, 0.20865461230278015]]           │ [[0.21036866307258606, -0.00779180321842432, 0.22745515406131744, -0.015750668942928314, -0.256397545337677, 0.13391438126564026, 0.11667126417160034, 0.3859782814979553, 0.2547174096107483, -0.14783094823360443, -0.04466332122683525, 0.11520984023809433, -0.269272118806839, -0.3270541727542877, -0.07921270281076431, -0.150549978017807, 0.15999777615070343, 0.13606449961662292, 0.15092892944812775, 0.15102607011795044, 0.37776821851730347, 0.08148275315761566, -0.13754872977733612, -0.002833236940205097, 0.27818813920021057, -0.11708851158618927, -0.16202117502689362, 0.23150834441184998, -0.03557101637125015, -0.017456643283367157, 0.20416811108589172, -0.3160710334777832]]     │ [[-0.17663319408893585, -0.22108863294124603, 0.4587824046611786, -0.49858662486076355, -0.9431205987930298, 0.011264199391007423, 1.0128498077392578, -0.06053952872753143, -0.2750937044620514, 1.0834124088287354, 0.27615147829055786, -0.11863964796066284, -0.3384254574775696, -0.1720460057258606, 0.4972921311855316, 0.47098851203918457, -0.858792781829834, -0.07193741202354431, -0.7294318079948425, -0.2702735960483551, -0.3601093888282776, -0.17425549030303955, 0.684476912021637, 0.6079043745994568, 0.3315718173980713, -0.004031656309962273, -0.05171617120504379, -0.293827086687088, -0.3947853744029999, 0.07415000349283218, -0.6836329102516174, -0.03450194001197815]]     │ [[0.23829159140586853, 0.26274266839027405, 0.009924925863742828, 0.4225965440273285, 0.43451133370399475, 0.024104811251163483, 0.19767233729362488, 0.35494258999824524, 0.1291080117225647, 0.5742315053939819, -0.22098910808563232, -0.1519172042608261, -0.012450039386749268, -0.8833269476890564, -0.30055657029151917, -0.24027809500694275, 0.01297324150800705, 0.5435715317726135, 0.06416692584753036, 0.14230167865753174, -0.23540550470352173, -0.7721962928771973, -0.055218495428562164, -0.1019195094704628, -0.5082883834838867, -0.5152406096458435, -0.40606689453125, 0.39363375306129456, -0.20704379677772522, -0.4469844102859497, 0.28353452682495117, -0.06479734182357788]]   │ [[0.7567316293716431, -0.14118076860904694, -0.46936917304992676, -0.5319043397903442, 0.4474579691886902, 0.06231474503874779, -0.09730455279350281, -0.2630927562713623, 0.21369332075119019, 0.6679710149765015, -0.28631871938705444, 0.12117073684930801, 0.17842644453048706, 0.2205100804567337, 0.2349793016910553, 0.4139981269836426, 0.2720732092857361, 0.1674487292766571, -0.2769981622695923, 0.051978204399347305, -0.17394788563251495, 0.23172080516815186, 0.39294540882110596, 0.04010869562625885, 0.014318928122520447, 0.06483472883701324, -0.19127528369426727, -0.4711560010910034, -0.170837864279747, -0.14273430407047272, -0.02220800518989563, 0.40346163511276245]]         │ [[-0.019434262067079544, 0.08177691698074341, 0.029389552772045135, -0.27857154607772827, 0.4951237738132477, -0.15500158071517944, -0.3885105848312378, -0.1405363827943802, 0.0731457769870758, -0.05101954936981201, 0.08556503057479858, 0.3258374035358429, -0.30595219135284424, 0.051584355533123016, -0.1103721633553505, -0.16689884662628174, -0.24151495099067688, -0.3338651657104492, -0.37401431798934937, -0.18264764547348022, -0.3376145660877228, -0.37022680044174194, -0.17567090690135956, 0.010858029127120972, 0.04399555176496506, -0.16683894395828247, 0.07334408164024353, -0.11832432448863983, 0.008261546492576599, -0.058920666575431824, -0.01448027789592743, 8.662790060043335e-05]]     │\n",
      "│  3 │  7_1 │ 7_1_12.png │ [[0.1290624439716339, 0.19674527645111084, 0.07936710119247437, 0.11997106671333313, 0.09256452322006226, 0.04939476400613785, 0.14665725827217102, 0.040256060659885406, 0.09004487842321396, -0.0022241324186325073, 0.019548192620277405, 0.07382743060588837, 0.033747583627700806, 0.1877148449420929, 0.18447554111480713, 0.09472598135471344, 0.18113070726394653, 0.19394582509994507, 0.05019237846136093, 0.0059134140610694885, 0.14919480681419373, 0.07603517919778824, 0.22723059356212616, 0.21247413754463196, 0.25090491771698, -0.0028792917728424072, 0.008974403142929077, 0.1197778731584549, 0.16619765758514404, 0.16281671822071075, 0.20658975839614868, 0.25826799869537354]]   │ [[0.3020085096359253, 0.06212484464049339, 0.22005268931388855, 0.04945064336061478, 0.026326753199100494, -0.17081841826438904, 0.07768508791923523, -0.3245101869106293, 0.177455872297287, -0.2975427806377411, 0.19643914699554443, 0.2921838164329529, -0.28438800573349, -0.2926899790763855, -0.04872070625424385, -0.062305960804224014, -0.03289394825696945, 0.03565160185098648, 0.010411673225462437, -0.07590451091527939, 0.06165982782840729, 0.06987611949443817, 0.13224685192108154, 0.05238988250494003, 0.223275825381279, 0.07746585458517075, -0.1891203373670578, 0.3596278429031372, -0.198533833026886, 0.09614942967891693, 0.31215083599090576, -0.15605603158473969]]               │ [[0.2172091007232666, -0.5811537504196167, -0.2342292219400406, 0.5913701057434082, 0.6410874128341675, -0.30680790543556213, -0.13642817735671997, -0.05949205160140991, -0.12590532004833221, -0.2769124209880829, 0.09843815118074417, 0.22058948874473572, 0.3740420937538147, -0.23186078667640686, 0.23194646835327148, 0.6041104197502136, -0.10981172323226929, 0.029230238869786263, 0.06079290062189102, -0.062286362051963806, 1.2456490993499756, -0.23277953267097473, -0.37876927852630615, -0.3062869906425476, -0.2720951437950134, 0.08637041598558426, 0.21891413629055023, -0.21970932185649872, -0.5283128023147583, 0.8794031739234924, -0.18911509215831757, 0.9432075023651123]]  │ [[0.01682022213935852, 0.15574216842651367, 0.19239619374275208, 0.06430204957723618, 0.13460072875022888, -0.4410589337348938, -0.22903266549110413, 0.2624916434288025, 0.7368893027305603, 0.14814746379852295, -0.19350871443748474, -0.029891006648540497, 0.5022007822990417, -0.6459600329399109, 0.1909712553024292, 0.02823653817176819, 0.1505221426486969, -0.1259261965751648, -0.46246007084846497, 0.2853541672229767, -0.3687075078487396, -0.13480129837989807, -0.21544480323791504, 0.6102220416069031, -0.31388160586357117, 0.1664087176322937, -0.37952810525894165, -0.2631128132343292, -0.5688760876655579, -0.2041199505329132, -0.31512850522994995, -0.3037158250808716]]       │ [[0.12405859678983688, -0.05523787438869476, 0.3684886693954468, -0.2894315719604492, 0.13362213969230652, -0.12696583569049835, 0.3700636029243469, 0.5443595051765442, 0.06481218338012695, -0.09264644980430603, 0.022557668387889862, 0.399874746799469, 0.11747909337282181, 0.1638551652431488, 0.6059134602546692, 0.6737770438194275, 0.3006322979927063, 0.2683069109916687, 0.006465848535299301, 0.24768812954425812, 0.274389386177063, 0.05499674379825592, 0.03229420632123947, 0.7160117030143738, 0.12189784646034241, 0.4074671268463135, 0.2724820375442505, -0.40890413522720337, -0.5224049091339111, 0.0005467981100082397, -0.4332767128944397, 0.5145567059516907]]                  │ [[0.3297439515590668, -0.12446973472833633, -0.07352203875780106, -0.2432047426700592, 0.1926346719264984, -0.10096551477909088, -0.10184833407402039, -0.2672876715660095, 0.1842195689678192, -0.3031942844390869, -0.18743324279785156, -0.06902223825454712, -0.2705736756324768, -0.03530081361532211, -0.2255411595106125, 0.32963404059410095, -0.19649045169353485, -0.39389970898628235, -0.22095835208892822, -0.3703983724117279, -0.013042561709880829, -0.416989266872406, -0.6210235357284546, -0.08232812583446503, 0.06667111814022064, -0.08669203519821167, 0.015156298875808716, -0.053832679986953735, -0.2079431414604187, -0.6343517303466797, 0.207878977060318, 0.23426532745361328]]              │\n",
      "│  4 │  7_1 │ 7_1_13.png │ [[0.07395485043525696, 0.17124620079994202, 0.19579797983169556, 0.12026062607765198, 0.058909155428409576, 0.010370604693889618, 0.11220352351665497, 0.09058623760938644, 0.1342630684375763, 0.0705256462097168, 0.097685307264328, 0.012845836579799652, 0.0897669792175293, 0.14733099937438965, 0.19717320799827576, 0.12602050602436066, 0.2029418647289276, 0.17009006440639496, 0.14528529345989227, -0.00635828822851181, 0.15598154067993164, 0.06912769377231598, 0.13200680911540985, 0.0745268315076828, 0.1372499316930771, 0.02753320336341858, 0.09148446470499039, 0.13741154968738556, 0.12486551702022552, 0.1320999562740326, 0.1971224844455719, 0.21278902888298035]]               │ [[0.3928568661212921, 0.08067142963409424, 0.21605129539966583, -0.05470757558941841, -0.24569423496723175, 0.060131803154945374, 0.0018125250935554504, 0.11573968827724457, 0.33105096220970154, -0.2832849323749542, -0.0452398806810379, 0.2591085433959961, -0.3328525424003601, -0.3052808940410614, 0.1440291404724121, -0.1388002336025238, 0.18706408143043518, -0.3781883120536804, 0.3030875027179718, 0.037469882518053055, 0.3525150418281555, 0.15624946355819702, -0.04488346725702286, 0.0847223773598671, -0.0941571444272995, 0.20317116379737854, 0.0990661233663559, 0.2963828444480896, 0.08096027374267578, 0.4270645081996918, 0.3809156119823456, -0.1188744306564331]]                 │ [[0.18069368600845337, -0.038399189710617065, -0.24364706873893738, 0.1294734925031662, -0.49926212430000305, -0.3807235360145569, 0.06139473617076874, -0.08144941180944443, 0.43795838952064514, 0.4112480878829956, -0.5117345452308655, -0.07153545320034027, -0.14949409663677216, -0.33890998363494873, 0.25774475932121277, -0.09117846190929413, 0.1782490760087967, 0.17067430913448334, -0.16140073537826538, 0.22218357026576996, 0.4407826066017151, 0.6320415139198303, -0.76362544298172, 0.20134052634239197, -0.3084929585456848, -0.3950938582420349, -0.005303343757987022, 0.2642751932144165, 0.014336390420794487, -0.03880305588245392, 0.06536122411489487, 0.36188018321990967]] │ [[0.08189915865659714, -0.49590593576431274, 0.7497208714485168, -0.1051008552312851, -0.45616796612739563, -0.49440649151802063, -0.4529076814651489, -0.07728105783462524, 0.35001516342163086, 0.15296527743339539, 0.11537245661020279, 0.3895761966705322, 0.21602851152420044, -0.26450783014297485, 0.5868805050849915, 0.013067781925201416, -0.24253138899803162, -0.013394355773925781, -0.2176591455936432, -0.35506105422973633, 0.09254937618970871, -0.2406330704689026, -0.28480264544487, -0.018627014011144638, -0.3151708245277405, 0.22070497274398804, -0.006714023649692535, 0.30449557304382324, 0.2807098925113678, 0.4719165861606598, 0.2613910138607025, 0.010669596493244171]]  │ [[-0.010827787220478058, -0.05440230667591095, 0.2862014174461365, -0.3122594952583313, 0.3257378935813904, 0.03778836503624916, 0.24722230434417725, -0.1184263527393341, 0.6622621417045593, 0.6139227747917175, -0.1917904019355774, 0.03278501331806183, 0.2514270842075348, -0.37216508388519287, 0.487527072429657, 0.32740145921707153, 0.07422883808612823, 0.034521397203207016, 0.42755764722824097, 0.03320162743330002, -0.08639651536941528, -0.06588616967201233, 0.458354651927948, 0.5633505582809448, -0.046503596007823944, 0.46195900440216064, -0.4025997519493103, -0.42209136486053467, -0.038514167070388794, -0.1917811930179596, 0.12074452638626099, 0.184956356883049]]          │ [[-0.2336520105600357, -0.36249321699142456, -0.22490733861923218, -0.33498862385749817, 0.036130860447883606, -0.1776350438594818, -0.17736279964447021, -0.0984424352645874, -0.11415170133113861, -0.14121603965759277, 0.11791276931762695, -0.123877614736557, -0.09793616831302643, -0.37011703848838806, -0.22316063940525055, -0.5275150537490845, -0.20772749185562134, -0.05306793749332428, 0.15621252357959747, -0.015512704849243164, -0.21310129761695862, -0.12474669516086578, 0.1488831639289856, 0.04587342590093613, 0.3111840784549713, -0.04655691981315613, -0.056567758321762085, -0.16537348926067352, -0.05531187355518341, 0.013830989599227905, -0.07658058404922485, -0.3045654594898224]]     │\n",
      "│  5 │  7_1 │ 7_1_14.png │ [[0.11986999958753586, 0.2613877058029175, 0.24104303121566772, 0.12033823132514954, 0.02886463701725006, 0.024809949100017548, 0.13306179642677307, 0.021034426987171173, 0.20539341866970062, 0.07027067989110947, 0.102084219455719, -0.02409079670906067, 0.18690451979637146, 0.1385124772787094, 0.08285880833864212, 0.08958464860916138, 0.21829251945018768, 0.15337365865707397, 0.10121844708919525, -0.03323271870613098, 0.1449621021747589, 0.042479149997234344, 0.18929435312747955, 0.09799396246671677, 0.09554621577262878, -0.010697945952415466, 0.14249344170093536, 0.08732530474662781, 0.11996454000473022, 0.13161376118659973, 0.2108355164527893, 0.23982232809066772]]        │ [[0.32322949171066284, 0.3870925009250641, 0.3446950614452362, -0.01028029341250658, 0.06194362789392471, 0.14035546779632568, 0.11864796280860901, 0.13729460537433624, 0.5065280795097351, -0.039559442549943924, 0.22744880616664886, 0.05900963395833969, -0.32540249824523926, -0.2301187664270401, -0.052337292581796646, -0.2664068937301636, 0.23920343816280365, 0.49678683280944824, 0.15460671484470367, 0.08307670801877975, 0.15380579233169556, 0.06752154976129532, 0.21706807613372803, 0.0634024515748024, 0.2552528977394104, 0.04403604939579964, -0.11151213943958282, -0.033680740743875504, -0.006635666824877262, 0.17844551801681519, 0.4484238922595978, -0.10668138414621353]]        │ [[-0.5174016952514648, 0.06639213860034943, -0.2839932441711426, 0.37510716915130615, 0.15199215710163116, -0.5549275875091553, 0.05195463448762894, 0.22150318324565887, 0.3342306911945343, -0.03942517191171646, 0.09694108366966248, -0.583041250705719, 0.009394155815243721, 0.023672619834542274, 0.113053098320961, 0.2883181571960449, -0.09824106842279434, 0.3569883704185486, -0.5978481769561768, -0.021271029487252235, -0.002833819016814232, -0.3601059913635254, -0.6489844918251038, -0.4287709891796112, -0.47818925976753235, 0.043433837592601776, 0.33412376046180725, 0.6742937564849854, 0.4599177837371826, -0.3213341534137726, 0.12370240688323975, 0.013795027509331703]]    │ [[0.07855682820081711, 0.5559152364730835, -0.01716822385787964, 0.35680559277534485, -0.25298169255256653, -1.400273323059082, 0.8970451951026917, 0.32783418893814087, 0.42684635519981384, -0.4016171097755432, 0.05639345198869705, 0.4474785625934601, 0.30468645691871643, -0.5563269257545471, -0.5674346685409546, 0.5592086911201477, -0.3302899897098541, -0.7737728953361511, 0.6235840320587158, 0.13513121008872986, 0.060136742889881134, -0.14112380146980286, 0.10329807549715042, -0.5091143250465393, -0.10320381820201874, -0.26342833042144775, 0.03656719624996185, -0.12903901934623718, -0.7349728345870972, 0.24463480710983276, -0.016154147684574127, -0.6459307074546814]]      │ [[0.16903482377529144, -0.6278709173202515, 0.04921767860651016, -0.4205532670021057, 0.5652830004692078, 0.03787395730614662, 0.09297055751085281, 0.12531517446041107, 0.3942378759384155, 0.4337456226348877, 0.021436244249343872, 0.4337524175643921, 0.2241704761981964, 0.07655485719442368, 0.23910929262638092, 0.5999529957771301, 0.5044508576393127, 0.2875533401966095, 0.025716066360473633, -0.24979491531848907, 0.06570684164762497, 0.04472145065665245, 0.4341999888420105, 0.6494714617729187, -0.0045145004987716675, 0.686937689781189, -0.0037686750292778015, -0.45053237676620483, -0.3720417618751526, -0.41455942392349243, -0.2608286142349243, 0.4524409770965576]]            │ [[-0.314784973859787, -0.12165239453315735, -0.3103296458721161, -0.1485813558101654, 0.08553194999694824, -0.341878741979599, -0.29791635274887085, -0.03632497414946556, 0.026840537786483765, 0.21797087788581848, 0.003428161144256592, 0.35019001364707947, -0.2614702582359314, 0.1112639456987381, -0.013950169086456299, -0.22518327832221985, -0.12396147847175598, -0.3874404728412628, -0.2883647382259369, -0.1710934340953827, 0.0667276531457901, 0.004755347967147827, -0.03593556582927704, 0.04189818352460861, 0.5576428174972534, -0.1732964813709259, 0.04411545395851135, 0.013370856642723083, -0.25157099962234497, -0.31840449571609497, 0.24390017986297607, 0.020184874534606934]]               │\n",
      "│  6 │  7_1 │ 7_1_15.png │ [[0.11289139837026596, 0.1836080402135849, 0.18870604038238525, 0.09803176671266556, 0.023198336362838745, 0.0743405818939209, 0.19071054458618164, 0.06597825139760971, 0.17779207229614258, 0.08992913365364075, 0.12805768847465515, -0.02105177938938141, 0.14653564989566803, 0.1677907556295395, 0.09892140328884125, 0.12482364475727081, 0.18560761213302612, 0.11648878455162048, 0.13034701347351074, 0.04872190207242966, 0.12299639731645584, 0.09583637118339539, 0.16096797585487366, 0.03520849347114563, 0.07314848899841309, 0.06548912823200226, 0.10097005218267441, 0.09042149782180786, 0.12125185877084732, 0.09036004543304443, 0.19431182742118835, 0.22514089941978455]]          │ [[0.15675601363182068, 0.3120550811290741, 0.05929015949368477, 0.06643848121166229, 0.04921095073223114, 0.1374426931142807, -0.0343000702559948, 0.09887345880270004, 0.26860111951828003, -0.17502793669700623, 0.13654878735542297, 0.08648456633090973, -0.4758569002151489, -0.2154799848794937, 0.03358032926917076, -0.13685818016529083, 0.059479814022779465, 0.13804838061332703, 0.1366627961397171, 0.0834488570690155, 0.09972080588340759, 0.13347205519676208, 0.030333418399095535, 0.24631065130233765, 0.24361546337604523, 0.0830368846654892, 0.011554398573935032, 0.07647083699703217, -0.043031882494688034, 0.08853007853031158, 0.06388863176107407, -0.0918409451842308]]            │ [[-0.41333475708961487, -0.6040496826171875, -0.6657122373580933, -0.799598217010498, -0.21211019158363342, 0.34782978892326355, 0.4114590287208557, 0.15455707907676697, 0.13674794137477875, 0.6595652103424072, -0.08088413625955582, 0.31811287999153137, 0.013393929228186607, 0.2340119630098343, 0.13646523654460907, 0.10402241349220276, -0.3569376468658447, 0.4021904766559601, -0.18294879794120789, 0.5930576920509338, 0.009341971948742867, -0.18108901381492615, -0.3987235426902771, -0.4869133532047272, -0.8820000290870667, 0.6417434215545654, 0.42990022897720337, 0.6995960474014282, 0.10376128554344177, 0.4923190176486969, 0.0658349096775055, 0.039174117147922516]]         │ [[0.06064654141664505, 0.8111370205879211, 0.10809846967458725, -0.060842715203762054, -0.3229745626449585, -0.7364022135734558, 0.02841746062040329, 0.10124965757131577, 0.3182782232761383, -0.028296317905187607, -0.550971508026123, -0.2824346423149109, 0.035936422646045685, -0.5005874633789062, -0.37247031927108765, -0.2930196523666382, -0.12405791878700256, 0.1539004147052765, 0.0519360676407814, -0.8613057136535645, -0.11794395744800568, -0.883094310760498, -0.09161049127578735, -0.5673478245735168, 0.4680233895778656, -0.12259764224290848, -0.47796645760536194, -0.294626921415329, -0.47401049733161926, 0.35710176825523376, 0.3287695348262787, -0.3149511218070984]]      │ [[0.29254913330078125, -0.24603186547756195, -0.09124621748924255, -0.635471761226654, -0.22512434422969818, 0.13250432908535004, 0.034556418657302856, 0.638487696647644, 0.4951167702674866, -0.046971291303634644, 0.19526901841163635, 0.7334759831428528, 0.7076977491378784, -0.21074630320072174, -0.19552135467529297, -0.1748688817024231, -0.15239207446575165, 0.5786633491516113, 0.19706286489963531, -0.13695266842842102, 0.752723753452301, -0.1883990317583084, 0.4821600914001465, 0.11325147747993469, 0.18526901304721832, -0.34617024660110474, -0.11129148304462433, -0.28725647926330566, 0.1747940182685852, 0.03947613388299942, -0.2106536477804184, 0.6401423811912537]]         │ [[-0.18505127727985382, -0.23062488436698914, -0.24324829876422882, -0.42619436979293823, -0.017178021371364594, 0.011943310499191284, -0.1081949770450592, -0.34239307045936584, -0.014602173119783401, -0.30755436420440674, -0.13764676451683044, 0.1341523826122284, -0.08445639908313751, -0.02240704745054245, 0.019140422344207764, -0.21260309219360352, 0.31745487451553345, 0.03676000237464905, -0.0037239491939544678, -0.13912004232406616, 0.10990042984485626, -0.15760082006454468, 0.12024866044521332, -0.23557552695274353, 0.3055992126464844, -0.1570974588394165, 0.20247873663902283, -0.04727602005004883, -0.2906637191772461, -0.01732197403907776, -0.11173280328512192, -0.23246265947818756]] │\n",
      "│  7 │  7_1 │ 7_1_16.png │ [[0.13319431245326996, 0.2085704505443573, 0.1685207337141037, 0.08999422937631607, 0.01633525639772415, 0.046573489904403687, 0.14593394100666046, 0.06475909054279327, 0.18643608689308167, 0.0860571414232254, 0.10446009039878845, 0.02480291575193405, 0.09366796165704727, 0.14093086123466492, 0.10514946281909943, 0.07769645750522614, 0.21530860662460327, 0.13571490347385406, 0.09652527421712875, -0.025621190667152405, 0.11648611724376678, 0.06850410997867584, 0.1274747997522354, 0.13864833116531372, 0.12884992361068726, 0.06563806533813477, 0.09878502786159515, 0.08003321290016174, 0.12433310598134995, 0.07123151421546936, 0.2225034087896347, 0.22263193130493164]]           │ [[0.28495773673057556, 0.040407124906778336, 0.13745735585689545, -0.0821194127202034, -0.21930788457393646, -0.12246522307395935, 0.23856881260871887, 0.03268615901470184, 0.3086632490158081, 0.13742688298225403, 0.06416624784469604, -0.034217432141304016, -0.0713992565870285, -0.2714797258377075, -0.04686162620782852, -0.06285516172647476, 0.16043721139431, 0.016338106244802475, 0.07368443161249161, -0.03591639921069145, 0.021496331319212914, 0.14572931826114655, -0.025030754506587982, -0.05847438797354698, 0.2612129747867584, 0.312946617603302, -0.29501041769981384, -0.04094235971570015, -0.22374947369098663, 0.11730748414993286, 0.16142643988132477, -0.0985875353217125]]     │ [[-0.1263323724269867, -0.5988245010375977, -0.35211214423179626, -0.12090332806110382, 0.5357732772827148, -0.45723602175712585, 0.5285154581069946, 0.06753029674291611, 0.18808232247829437, -0.4455982744693756, -0.2511958181858063, 0.18604589998722076, -0.756525993347168, -0.43696901202201843, 0.07979898899793625, -0.11820794641971588, -0.3809792399406433, 0.6149014234542847, -0.4837891161441803, 0.0673646628856659, 0.24009113013744354, 0.7291092276573181, -0.41742271184921265, -0.5938826203346252, 0.22495834529399872, -0.04516002535820007, -0.38275018334388733, 0.33661535382270813, 0.6126516461372375, 0.35849907994270325, 0.26844078302383423, 0.41079601645469666]]      │ [[-0.46092313528060913, 0.5186387300491333, -0.7242527604103088, 0.23102110624313354, 0.051724307239055634, -0.47463899850845337, 0.46230682730674744, -0.25793278217315674, 1.044574499130249, -0.15469661355018616, 0.1948578655719757, 0.2603261470794678, 0.08354581147432327, -0.4455621838569641, 0.3772833049297333, 0.23506614565849304, 0.03731640428304672, -0.3352730870246887, 0.40563926100730896, -0.6010690927505493, -0.15059345960617065, -0.5339958667755127, -0.19693061709403992, 0.4075148105621338, 0.07357608526945114, -0.49166446924209595, -0.8057636022567749, -0.19782297313213348, -0.5329251289367676, 0.511962890625, -0.20431548357009888, 0.2702209949493408]]            │ [[0.231759712100029, -0.6321549415588379, -0.06993342190980911, -0.5345269441604614, -0.18576230108737946, -0.07774756848812103, -0.02388298511505127, -0.06304732710123062, 0.23264284431934357, 0.2791372537612915, 0.08397211134433746, 0.029594026505947113, -0.17954619228839874, 0.36786097288131714, 0.16891349852085114, 0.38007813692092896, 0.14600928127765656, 0.6284220814704895, -0.09227599203586578, -0.18074969947338104, -0.06415316462516785, 0.4274659752845764, 0.34451407194137573, 0.5730219483375549, -0.07784996926784515, -0.15707889199256897, 0.06314114481210709, -0.575473427772522, -0.062161631882190704, -0.19824469089508057, -0.008722037076950073, 0.6061527729034424]] │ [[-0.12191325426101685, -0.06933162361383438, -0.11954095214605331, -0.03934028744697571, 0.055200040340423584, -0.021176576614379883, -0.18518933653831482, -0.14305438101291656, -0.3935127854347229, -0.27917513251304626, 0.02842652052640915, -0.17306450009346008, -0.09227031469345093, 0.3102908432483673, -0.12960746884346008, -0.44984471797943115, -0.28695929050445557, 0.020766422152519226, -0.05607032775878906, -0.03958535194396973, -0.08078545331954956, 0.06185145676136017, 0.09460939466953278, -0.04224998503923416, -0.01723596826195717, 0.05985859036445618, -0.21744981408119202, -0.055465325713157654, -0.05482320487499237, 0.1453818678855896, -0.13106845319271088, -0.1880154311656952]] │\n",
      "│  8 │  7_1 │ 7_1_17.png │ [[0.16996872425079346, 0.23115040361881256, 0.16006100177764893, 0.126348614692688, 0.025197535753250122, -0.01558174192905426, 0.13010357320308685, 0.07875777781009674, 0.19428768754005432, 0.020382821559906006, 0.13118021190166473, 0.06516045331954956, 0.08041100949048996, 0.17727860808372498, 0.16912217438220978, 0.10219756513834, 0.2061498761177063, 0.11196894943714142, 0.12040073424577713, 0.009894534945487976, 0.058995649218559265, 0.021241143345832825, 0.11663047224283218, 0.09534202516078949, 0.11715910583734512, 0.06845565140247345, 0.03906535357236862, 0.14569689333438873, 0.08160895109176636, 0.11448600143194199, 0.23890939354896545, 0.232681006193161]]           │ [[0.11551633477210999, 0.2654661238193512, 0.13803993165493011, -0.08771441131830215, -0.16413195431232452, 0.0909201130270958, -0.0699109137058258, -0.13803459703922272, 0.2449151873588562, -0.12607987225055695, 0.14267277717590332, -0.010460554622113705, 0.005370534025132656, -0.1375473588705063, 0.01383719127625227, -0.15279269218444824, -0.0653185248374939, 0.4078971743583679, 0.015246964059770107, -0.02068214863538742, 0.23019634187221527, 0.21092969179153442, 0.14984020590782166, -0.019610218703746796, 0.1897917240858078, 0.13155516982078552, -0.07322590053081512, 0.08953719586133957, -0.20105184614658356, -0.27382317185401917, 0.4002239406108856, -0.27675187587738037]]    │ [[-0.48739388585090637, -0.5590922236442566, 0.39478620886802673, -0.08384642004966736, -0.6286087036132812, -0.007069965824484825, -0.3009950518608093, 0.32717472314834595, 0.22623518109321594, 0.005921201780438423, 0.02778559736907482, 0.3333083391189575, 0.09148772805929184, 0.956252932548523, 0.6613063812255859, 0.564522922039032, 0.23591114580631256, 0.2385602742433548, -0.36747416853904724, -0.3484070897102356, 0.04496292769908905, -0.2760021984577179, 0.051991090178489685, -0.29137182235717773, 0.18883086740970612, -0.042414456605911255, 0.7421154975891113, 0.039794981479644775, -0.2600301504135132, -0.5689000487327576, -0.032969504594802856, -0.2684189975261688]]  │ [[0.44217589497566223, -0.0639854371547699, 0.07891290634870529, -0.3222423493862152, -0.6477276682853699, -0.37491506338119507, -0.06659774482250214, 0.001438632607460022, -0.19863295555114746, 0.23462635278701782, 0.0725119486451149, -0.28709521889686584, -0.2153463065624237, -0.5629078149795532, -0.09628228843212128, -0.12383749336004257, 0.3342975676059723, 0.0010465532541275024, -0.2996685802936554, 0.11480114609003067, 0.1899183988571167, -0.4049778878688812, 0.42032331228256226, -0.674364447593689, 0.28575775027275085, -0.2153787612915039, -0.28833088278770447, -0.6722785830497742, -0.1273323893547058, -0.25656723976135254, 0.19699028134346008, -0.22151419520378113]] │ [[-0.12028691172599792, 0.025217490270733833, -0.1260252445936203, 0.059629835188388824, 0.31455105543136597, -0.14071835577487946, -0.1256423145532608, 0.3201131820678711, 0.7061786651611328, -0.030719228088855743, -0.07446806132793427, -0.1829187422990799, 0.3129838705062866, 0.001387670636177063, 0.6134037971496582, 0.2261512279510498, 0.29788655042648315, 0.6058142185211182, -0.10881516337394714, -0.053446151316165924, 0.04758535325527191, 0.20716579258441925, 0.276720255613327, 0.14684155583381653, 0.2892724275588989, 0.28451210260391235, 0.1256939172744751, -0.8421638011932373, 0.05193693935871124, -0.10831166803836823, -0.3638926148414612, 0.2843886613845825]]         │ [[-0.2425512820482254, -0.08855492621660233, -0.08672873675823212, -0.4731243848800659, 0.3077424466609955, 0.05909945070743561, -0.31188157200813293, 0.09351404011249542, -0.3315296769142151, -0.208917498588562, 0.21732407808303833, 0.07035019993782043, 0.021698735654354095, -0.07118543982505798, 0.14777666330337524, -0.1140938252210617, -0.2278481125831604, -0.11868329346179962, -0.025384187698364258, -0.21449673175811768, 0.012531355023384094, -0.05450843274593353, -0.23044012486934662, -0.11903883516788483, 0.3037499785423279, -0.052548348903656006, -0.1781565099954605, -0.11466174572706223, -0.20331357419490814, 0.33168506622314453, 0.12874466180801392, 0.014376528561115265]]          │\n",
      "│  9 │  7_1 │ 7_1_18.png │ [[0.047477930784225464, 0.16537654399871826, 0.1043805181980133, 0.07688131928443909, 0.016731224954128265, -0.028681814670562744, 0.17674948275089264, 0.027739472687244415, 0.09311016649007797, 0.08274409174919128, 0.08468243479728699, 0.05345524102449417, 0.031301841139793396, 0.16574542224407196, 0.24489262700080872, 0.08053983002901077, 0.18668723106384277, 0.11709296703338623, 0.10385124385356903, -0.010716751217842102, 0.1504848599433899, 0.04887255281209946, 0.11059999465942383, 0.08694469183683395, 0.10500071942806244, 0.0003099516034126282, 0.002302825450897217, 0.12649452686309814, 0.1673104465007782, 0.17467235028743744, 0.29035890102386475, 0.22350819408893585]] │ [[0.12198908627033234, 0.2357395887374878, 0.14374512434005737, -0.01950943097472191, -0.29610490798950195, -0.02579469606280327, 0.40917396545410156, -0.2064349502325058, 0.125369593501091, -0.02249303087592125, 0.014386639930307865, 0.048077017068862915, -0.35629329085350037, -0.3256482481956482, 0.09960853308439255, 0.1309458464384079, 0.011250381357967854, 0.14491412043571472, -0.09824328869581223, -0.09338540583848953, 0.08209022134542465, 0.17128030955791473, -0.07375333458185196, 0.033686522394418716, 0.18216413259506226, 0.3583357632160187, 0.044270262122154236, 0.008551164530217648, -0.04486571252346039, -0.026036981493234634, 0.40836676955223083, -0.14928728342056274]] │ [[-0.3356392979621887, -0.23554755747318268, -0.053245604038238525, 0.3391960561275482, 0.08699105679988861, -0.06916224956512451, 0.40017321705818176, 0.4367687404155731, 0.248219832777977, 0.8589286804199219, 0.48423728346824646, -0.09009765833616257, -0.16107115149497986, 0.12844763696193695, -0.7451418042182922, -0.0493893176317215, -0.2138146609067917, 0.20958448946475983, -0.333034873008728, 0.07564599066972733, 0.07379796355962753, 0.1394548863172531, -0.3978576362133026, -0.39457693696022034, 0.49398186802864075, 0.3228567838668823, -0.6512314081192017, 0.5121958255767822, -0.6224181652069092, 0.41545048356056213, -0.3655110001564026, -0.2915189862251282]]         │ [[-0.2921414077281952, 0.33152127265930176, -0.6288092136383057, 0.41121670603752136, -0.19367188215255737, -0.6396565437316895, 0.4910792410373688, -0.05765828490257263, 0.14179980754852295, 0.17394724488258362, -0.18654710054397583, 0.11350785940885544, 0.5566359758377075, -0.7845694422721863, -0.17153996229171753, -0.1345522403717041, -0.2709508240222931, -0.17968997359275818, -0.21847131848335266, -0.2663920223712921, -0.2315683364868164, -0.3483845889568329, -0.2906123995780945, -0.33966127038002014, 0.6540218591690063, 0.48064425587654114, -0.9538097977638245, 0.413091778755188, 0.04650221765041351, -0.2433675229549408, 0.039568863809108734, 0.18565601110458374]]      │ [[0.37209010124206543, -0.17199717462062836, 0.028352539986371994, -0.24944628775119781, 0.0938914567232132, -0.03522757440805435, 0.25118815898895264, 0.26595309376716614, 0.4689764976501465, 0.15587733685970306, 0.5309348106384277, 0.3611118197441101, 0.28695258498191833, -0.5107834935188293, 0.5912850499153137, -0.2239728718996048, 0.15321743488311768, 0.22691738605499268, 0.17276546359062195, -0.026573561131954193, -0.32288724184036255, -0.05651246756315231, 0.5037887692451477, 0.363828182220459, 0.18299391865730286, -0.2911975383758545, -0.05595296621322632, -0.16740137338638306, 0.10378554463386536, -0.20410414040088654, 0.168297678232193, 0.15655075013637543]]         │ [[-0.018152374774217606, -0.16835670173168182, -0.1820969581604004, -0.2456321120262146, 0.10963243246078491, -0.08779431879520416, -0.2034127116203308, -0.19230681657791138, 0.15879586338996887, -0.17916756868362427, 0.08961060643196106, 0.30495110154151917, -0.08310475200414658, -0.012520726770162582, 0.21870416402816772, 0.22835278511047363, -0.16988679766654968, -0.22960931062698364, -0.24547220766544342, -0.24295037984848022, -0.008086666464805603, 0.08931916952133179, -0.026862025260925293, -0.2057848423719406, 0.0988035798072815, -0.08184230327606201, 0.05886650085449219, 0.28374630212783813, -0.03155156970024109, 0.2275765836238861, 0.08122783899307251, -0.30334174633026123]]       │\n",
      "╘════╧══════╧════════════╧════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╧════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('independent variable: (1)이미지 피쳐맵')\n",
    "print(tabulate(dataset[['id','Name','image_feature_1','image_feature_2','image_feature_3','image_feature_4','image_feature_5','image_feature_6']][:10], headers='keys', tablefmt='fancy_outline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "independent variable: (2)시험정보 데이터\n",
      "╒════╤═══════╤═══════════════════════════╤══════════════╤═══════════╤════════╕\n",
      "│    │    id │                      Name │   stress_mpa │   temp_oc │    LMP │\n",
      "╞════╪═══════╪═══════════════════════════╪══════════════╪═══════════╪════════╡\n",
      "│  0 │ A0410 │  210330-410-2_m005_r1.png │       263.38 │       900 │ 26.666 │\n",
      "│  1 │  A-C5 │  210324-AC5-2_m008_r1.png │        357.2 │       800 │ 25.526 │\n",
      "│  2 │  A-C1 │  210415-AC1-1_m013_r1.png │        302.3 │       800 │ 26.049 │\n",
      "│  3 │  A-C8 │  210415-AC8-1_m005_r1.png │         79.7 │      1000 │ 30.389 │\n",
      "│  4 │  A-C9 │  210330-AC9-4_m009_r1.png │          329 │       800 │ 25.857 │\n",
      "│  5 │   7_1 │                7_1_31.png │        181.5 │       900 │  27.75 │\n",
      "│  6 │  18_1 │               18_1_36.png │         38.4 │      1000 │  32.66 │\n",
      "│  7 │  A-C3 │  210324-AC3-4_m012_r1.png │         94.9 │       950 │ 29.799 │\n",
      "│  8 │ A-C10 │ 210406-AC10-1_m006_r1.png │        164.5 │       900 │ 28.142 │\n",
      "│  9 │  A-C9 │  210330-AC9-3_m002_r1.png │          329 │       800 │ 25.857 │\n",
      "╘════╧═══════╧═══════════════════════════╧══════════════╧═══════════╧════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('independent variable: (2)시험정보 데이터')\n",
    "print(tabulate(dataset[['id','Name','stress_mpa','temp_oc', \"LMP\"]].sample(frac=1).reset_index(drop=True)[:10], headers='keys', tablefmt='fancy_outline',  numalign='right',stralign='right' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "independent variable: (3)야금학적 물성정보\n",
      "╒════╤═══════╤══════════════════════════╤═════════╤══════════╤═════════════════╤════════════════╤═════════════════╕\n",
      "│    │    id │                     Name │   gamma │   gammaP │   gammaP_aspect │   gammaP_width │   gammaP_circle │\n",
      "╞════╪═══════╪══════════════════════════╪═════════╪══════════╪═════════════════╪════════════════╪═════════════════╡\n",
      "│  0 │ G-C11 │       950_g-c11_4_13.png │ 82.6375 │  17.3625 │         1.32894 │        15.9146 │         1.05955 │\n",
      "│  1 │  A-C2 │ 210416-AC2-1_m003_r1.png │ 53.3584 │  46.6416 │         2.72009 │        52.7805 │         0.44296 │\n",
      "│  2 │  17_2 │              17_2_23.png │ 56.7493 │  43.2507 │         1.14937 │        198.928 │         0.20721 │\n",
      "│  3 │ A0412 │ 210330-412-2_m010_r1.png │ 62.0824 │  37.9176 │         3.25611 │        41.5151 │        0.378775 │\n",
      "│  4 │   7_4 │               7_4_45.png │ 62.2771 │  37.7229 │         1.70116 │        33.8658 │        0.586627 │\n",
      "│  5 │   7_1 │               7_1_38.png │ 65.8421 │  34.1579 │         1.43137 │        40.4794 │        0.657277 │\n",
      "│  6 │  17_1 │              17_1_26.png │ 54.6936 │  45.3064 │         1.44259 │        196.002 │        0.238298 │\n",
      "│  7 │  G-C4 │        1000_g-c4_2_6.png │ 86.2406 │  13.7594 │         1.55022 │        37.0629 │        0.974877 │\n",
      "│  8 │   7_4 │                7_4_3.png │  64.502 │   35.498 │          1.5559 │        47.4092 │         0.50314 │\n",
      "│  9 │  18_4 │              18_4_44.png │ 53.7493 │  46.2507 │         1.56712 │        73.5287 │         0.48568 │\n",
      "╘════╧═══════╧══════════════════════════╧═════════╧══════════╧═════════════════╧════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('independent variable: (3)야금학적 물성정보')\n",
    "print(tabulate(dataset[['id','Name','gamma','gammaP','gammaP_aspect','gammaP_width','gammaP_circle']].sample(frac=1).reset_index(drop=True)[:10], headers='keys', tablefmt='fancy_outline', numalign='right', stralign='right' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependent variable: 열화수준\n",
      "╒════╤═══════╤══════════════════════════╤════════╤═════════╤═════════╕\n",
      "│    │    id │                     Name │   mean │   upper │   lower │\n",
      "╞════╪═══════╪══════════════════════════╪════════╪═════════╪═════════╡\n",
      "│  0 │  A-C8 │ 210415-AC8-4_m008_r1.png │    100 │     100 │  50.569 │\n",
      "│  1 │  A-C6 │ 210324-AC6-3_m014_r1.png │    100 │     100 │  47.714 │\n",
      "│  2 │   9_5 │               9_5_37.png │   0.49 │   1.027 │   0.234 │\n",
      "│  3 │  18_3 │               18_3_4.png │ 74.531 │     100 │  37.687 │\n",
      "│  4 │   7_5 │               7_5_22.png │ 10.541 │  22.095 │   5.029 │\n",
      "│  5 │  G-C2 │        900_g_c2_2_19.png │    100 │     100 │  47.714 │\n",
      "│  6 │ G0413 │       1000_g0413_3_8.png │    100 │     100 │  50.569 │\n",
      "│  7 │  17_5 │              17_5_42.png │ 11.482 │  22.708 │   5.806 │\n",
      "│  8 │   9_4 │               9_4_31.png │ 11.139 │  23.349 │   5.315 │\n",
      "│  9 │   7_4 │                7_4_5.png │ 31.589 │  66.212 │  15.071 │\n",
      "╘════╧═══════╧══════════════════════════╧════════╧═════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('dependent variable: 열화수준')\n",
    "print(tabulate(dataset[['id','Name','mean','upper','lower']].sample(frac=1).reset_index(drop=True)[:10], headers='keys', tablefmt='fancy_outline',numalign='right', stralign='right' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_x</th>\n",
       "      <th>id</th>\n",
       "      <th>Name</th>\n",
       "      <th>stress_mpa</th>\n",
       "      <th>temp_oc</th>\n",
       "      <th>LMP</th>\n",
       "      <th>mean</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower</th>\n",
       "      <th>gamma</th>\n",
       "      <th>...</th>\n",
       "      <th>gammaP_distrib</th>\n",
       "      <th>gammaP_aspect</th>\n",
       "      <th>gammaP_width</th>\n",
       "      <th>gammaP_circle</th>\n",
       "      <th>image_feature_1</th>\n",
       "      <th>image_feature_2</th>\n",
       "      <th>image_feature_3</th>\n",
       "      <th>image_feature_4</th>\n",
       "      <th>image_feature_5</th>\n",
       "      <th>image_feature_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in792sx_interrupt</td>\n",
       "      <td>7_1</td>\n",
       "      <td>7_1_1.png</td>\n",
       "      <td>181.5</td>\n",
       "      <td>900</td>\n",
       "      <td>27.75</td>\n",
       "      <td>21.674</td>\n",
       "      <td>45.429</td>\n",
       "      <td>10.34</td>\n",
       "      <td>74.264962</td>\n",
       "      <td>...</td>\n",
       "      <td>[90.964, 6.928, 2.108, 0.0, 0.0]</td>\n",
       "      <td>1.474768</td>\n",
       "      <td>33.981497</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>[[0.12024862319231033, 0.15066924691200256, 0....</td>\n",
       "      <td>[[0.11661333590745926, 0.13787168264389038, 0....</td>\n",
       "      <td>[[-0.2253749519586563, -0.2675281763076782, -0...</td>\n",
       "      <td>[[-0.3594183325767517, 0.003142327070236206, 0...</td>\n",
       "      <td>[[0.05846637487411499, -0.09212616086006165, 0...</td>\n",
       "      <td>[[-0.35045769810676575, -0.2591642439365387, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in792sx_interrupt</td>\n",
       "      <td>7_1</td>\n",
       "      <td>7_1_10.png</td>\n",
       "      <td>181.5</td>\n",
       "      <td>900</td>\n",
       "      <td>27.75</td>\n",
       "      <td>21.674</td>\n",
       "      <td>45.429</td>\n",
       "      <td>10.34</td>\n",
       "      <td>66.826782</td>\n",
       "      <td>...</td>\n",
       "      <td>[86.532, 12.121, 1.347, 0.0, 0.0]</td>\n",
       "      <td>1.478818</td>\n",
       "      <td>39.504702</td>\n",
       "      <td>0.698531</td>\n",
       "      <td>[[0.0669223964214325, 0.2487642467021942, 0.19...</td>\n",
       "      <td>[[0.1659088283777237, 0.29244664311408997, -0....</td>\n",
       "      <td>[[0.6055417060852051, 0.05654902756214142, 0.0...</td>\n",
       "      <td>[[-0.030698813498020172, 0.15434464812278748, ...</td>\n",
       "      <td>[[0.10532476007938385, -0.15585653483867645, 0...</td>\n",
       "      <td>[[-0.009201321750879288, -0.26059597730636597,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in792sx_interrupt</td>\n",
       "      <td>7_1</td>\n",
       "      <td>7_1_11.png</td>\n",
       "      <td>181.5</td>\n",
       "      <td>900</td>\n",
       "      <td>27.75</td>\n",
       "      <td>21.674</td>\n",
       "      <td>45.429</td>\n",
       "      <td>10.34</td>\n",
       "      <td>66.533290</td>\n",
       "      <td>...</td>\n",
       "      <td>[87.973, 8.935, 3.093, 0.0, 0.0]</td>\n",
       "      <td>1.575460</td>\n",
       "      <td>38.257919</td>\n",
       "      <td>0.650696</td>\n",
       "      <td>[[0.12086768448352814, 0.23515920341014862, 0....</td>\n",
       "      <td>[[0.21036866307258606, -0.00779180321842432, 0...</td>\n",
       "      <td>[[-0.17663319408893585, -0.22108863294124603, ...</td>\n",
       "      <td>[[0.23829159140586853, 0.26274266839027405, 0....</td>\n",
       "      <td>[[0.7567316293716431, -0.14118076860904694, -0...</td>\n",
       "      <td>[[-0.019434262067079544, 0.08177691698074341, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in792sx_interrupt</td>\n",
       "      <td>7_1</td>\n",
       "      <td>7_1_12.png</td>\n",
       "      <td>181.5</td>\n",
       "      <td>900</td>\n",
       "      <td>27.75</td>\n",
       "      <td>21.674</td>\n",
       "      <td>45.429</td>\n",
       "      <td>10.34</td>\n",
       "      <td>62.865252</td>\n",
       "      <td>...</td>\n",
       "      <td>[80.0, 17.358, 2.642, 0.0, 0.0]</td>\n",
       "      <td>1.536970</td>\n",
       "      <td>39.448273</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>[[0.1290624439716339, 0.19674527645111084, 0.0...</td>\n",
       "      <td>[[0.3020085096359253, 0.06212484464049339, 0.2...</td>\n",
       "      <td>[[0.2172091007232666, -0.5811537504196167, -0....</td>\n",
       "      <td>[[0.01682022213935852, 0.15574216842651367, 0....</td>\n",
       "      <td>[[0.12405859678983688, -0.05523787438869476, 0...</td>\n",
       "      <td>[[0.3297439515590668, -0.12446973472833633, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in792sx_interrupt</td>\n",
       "      <td>7_1</td>\n",
       "      <td>7_1_13.png</td>\n",
       "      <td>181.5</td>\n",
       "      <td>900</td>\n",
       "      <td>27.75</td>\n",
       "      <td>21.674</td>\n",
       "      <td>45.429</td>\n",
       "      <td>10.34</td>\n",
       "      <td>68.665074</td>\n",
       "      <td>...</td>\n",
       "      <td>[86.513, 12.5, 0.987, 0.0, 0.0]</td>\n",
       "      <td>1.435410</td>\n",
       "      <td>38.770547</td>\n",
       "      <td>0.676962</td>\n",
       "      <td>[[0.07395485043525696, 0.17124620079994202, 0....</td>\n",
       "      <td>[[0.3928568661212921, 0.08067142963409424, 0.2...</td>\n",
       "      <td>[[0.18069368600845337, -0.038399189710617065, ...</td>\n",
       "      <td>[[0.08189915865659714, -0.49590593576431274, 0...</td>\n",
       "      <td>[[-0.010827787220478058, -0.05440230667591095,...</td>\n",
       "      <td>[[-0.2336520105600357, -0.36249321699142456, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_x   id        Name  stress_mpa  temp_oc    LMP    mean  \\\n",
       "0  in792sx_interrupt  7_1   7_1_1.png       181.5      900  27.75  21.674   \n",
       "1  in792sx_interrupt  7_1  7_1_10.png       181.5      900  27.75  21.674   \n",
       "2  in792sx_interrupt  7_1  7_1_11.png       181.5      900  27.75  21.674   \n",
       "3  in792sx_interrupt  7_1  7_1_12.png       181.5      900  27.75  21.674   \n",
       "4  in792sx_interrupt  7_1  7_1_13.png       181.5      900  27.75  21.674   \n",
       "\n",
       "    upper  lower      gamma  ...                     gammaP_distrib  \\\n",
       "0  45.429  10.34  74.264962  ...   [90.964, 6.928, 2.108, 0.0, 0.0]   \n",
       "1  45.429  10.34  66.826782  ...  [86.532, 12.121, 1.347, 0.0, 0.0]   \n",
       "2  45.429  10.34  66.533290  ...   [87.973, 8.935, 3.093, 0.0, 0.0]   \n",
       "3  45.429  10.34  62.865252  ...    [80.0, 17.358, 2.642, 0.0, 0.0]   \n",
       "4  45.429  10.34  68.665074  ...    [86.513, 12.5, 0.987, 0.0, 0.0]   \n",
       "\n",
       "  gammaP_aspect  gammaP_width  gammaP_circle  \\\n",
       "0      1.474768     33.981497       0.505010   \n",
       "1      1.478818     39.504702       0.698531   \n",
       "2      1.575460     38.257919       0.650696   \n",
       "3      1.536970     39.448273       0.713092   \n",
       "4      1.435410     38.770547       0.676962   \n",
       "\n",
       "                                     image_feature_1  \\\n",
       "0  [[0.12024862319231033, 0.15066924691200256, 0....   \n",
       "1  [[0.0669223964214325, 0.2487642467021942, 0.19...   \n",
       "2  [[0.12086768448352814, 0.23515920341014862, 0....   \n",
       "3  [[0.1290624439716339, 0.19674527645111084, 0.0...   \n",
       "4  [[0.07395485043525696, 0.17124620079994202, 0....   \n",
       "\n",
       "                                     image_feature_2  \\\n",
       "0  [[0.11661333590745926, 0.13787168264389038, 0....   \n",
       "1  [[0.1659088283777237, 0.29244664311408997, -0....   \n",
       "2  [[0.21036866307258606, -0.00779180321842432, 0...   \n",
       "3  [[0.3020085096359253, 0.06212484464049339, 0.2...   \n",
       "4  [[0.3928568661212921, 0.08067142963409424, 0.2...   \n",
       "\n",
       "                                     image_feature_3  \\\n",
       "0  [[-0.2253749519586563, -0.2675281763076782, -0...   \n",
       "1  [[0.6055417060852051, 0.05654902756214142, 0.0...   \n",
       "2  [[-0.17663319408893585, -0.22108863294124603, ...   \n",
       "3  [[0.2172091007232666, -0.5811537504196167, -0....   \n",
       "4  [[0.18069368600845337, -0.038399189710617065, ...   \n",
       "\n",
       "                                     image_feature_4  \\\n",
       "0  [[-0.3594183325767517, 0.003142327070236206, 0...   \n",
       "1  [[-0.030698813498020172, 0.15434464812278748, ...   \n",
       "2  [[0.23829159140586853, 0.26274266839027405, 0....   \n",
       "3  [[0.01682022213935852, 0.15574216842651367, 0....   \n",
       "4  [[0.08189915865659714, -0.49590593576431274, 0...   \n",
       "\n",
       "                                     image_feature_5  \\\n",
       "0  [[0.05846637487411499, -0.09212616086006165, 0...   \n",
       "1  [[0.10532476007938385, -0.15585653483867645, 0...   \n",
       "2  [[0.7567316293716431, -0.14118076860904694, -0...   \n",
       "3  [[0.12405859678983688, -0.05523787438869476, 0...   \n",
       "4  [[-0.010827787220478058, -0.05440230667591095,...   \n",
       "\n",
       "                                     image_feature_6  \n",
       "0  [[-0.35045769810676575, -0.2591642439365387, -...  \n",
       "1  [[-0.009201321750879288, -0.26059597730636597,...  \n",
       "2  [[-0.019434262067079544, 0.08177691698074341, ...  \n",
       "3  [[0.3297439515590668, -0.12446973472833633, -0...  \n",
       "4  [[-0.2336520105600357, -0.36249321699142456, -...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 통합데이터\n",
    "\n",
    "dataset[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cho40\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\cho40\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\cho40\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\cho40\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "from data_openml import data_prep_openml\n",
    "import numpy as np\n",
    "\n",
    "feature_num = 1\n",
    "cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std, IF_train, IF_valid, IF_test, y_upper_train, y_upper_valid, y_upper_test, y_lower_train, y_lower_valid, y_lower_test = data_prep_openml(str(feature_num), datasplit=[.65, .15, .2])\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 900. ,  181.5],\n",
      "       [ 900. ,  181.5],\n",
      "       [ 900. ,  181.5],\n",
      "       ...,\n",
      "       [1000. ,   25.1],\n",
      "       [1000. ,   25.1],\n",
      "       [1000. ,   25.1]]), 'mask': array([[1, 1],\n",
      "       [1, 1],\n",
      "       [1, 1],\n",
      "       ...,\n",
      "       [1, 1],\n",
      "       [1, 1],\n",
      "       [1, 1]])}\n",
      "--------------------------------------------------\n",
      "{'data': array([[ 21.674],\n",
      "       [ 21.674],\n",
      "       [ 21.674],\n",
      "       ...,\n",
      "       [100.   ],\n",
      "       [100.   ],\n",
      "       [100.   ]])}\n",
      "--------------------------------------------------\n",
      "torch.Size([1, 32])\n",
      "tensor([[ 0.1202,  0.1507,  0.1546,  0.1247,  0.0643,  0.0926,  0.1591,  0.0743,\n",
      "          0.1405,  0.1670,  0.0844, -0.0227,  0.1007,  0.1309,  0.1843,  0.0965,\n",
      "          0.1741,  0.1247,  0.0965, -0.0221,  0.1345,  0.0791,  0.1235,  0.1331,\n",
      "          0.1097,  0.0371,  0.0495,  0.0930,  0.1544,  0.1267,  0.2695,  0.1939]])\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(\"-\"*50)\n",
    "print(y_train)\n",
    "print(\"-\"*50)\n",
    "print(IF_train['data'][0].shape)\n",
    "print(IF_train['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_openml import DataSetCatCon\n",
    "\n",
    "batchsize=216\n",
    "train_ds = DataSetCatCon(X_train, y_train, y_upper_train, y_lower_train, IF_train, cat_idxs, continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=batchsize, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, y_upper_valid, y_lower_valid, IF_valid, cat_idxs, continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=batchsize, shuffle=False,num_workers=4)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, y_upper_test, y_lower_test, IF_test, cat_idxs, continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=batchsize, shuffle=False,num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabAttention(\n",
       "  (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  (simple_MLP): ModuleList(\n",
       "    (0): simple_MLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): simple_MLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): RowColTransformer(\n",
       "    (embeds): Embedding(4, 32)\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=32, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.1, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=160, out_features=80, bias=True)\n",
       "      (1): Linear(in_features=80, out_features=40, bias=True)\n",
       "      (2): Linear(in_features=40, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embeds): Embedding(4, 32)\n",
       "  (mlpfory): simple_MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=1000, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1000, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models import TabAttention\n",
    "import torch\n",
    "\n",
    "y_dim = 1 # regression\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_size = 32\n",
    "transformer_depth = 6\n",
    "attention_heads = 8\n",
    "attention_dropout = 0.1\n",
    "ff_dropout = 0.1\n",
    "cont_embeddings = 'MLP'\n",
    "attentiontype = 'colrow'\n",
    "\n",
    "model = TabAttention(\n",
    "categories = tuple(cat_dims), \n",
    "num_continuous = len(con_idxs),                \n",
    "dim = embedding_size,                           \n",
    "dim_out = 1,                       \n",
    "depth = transformer_depth,                       \n",
    "heads = attention_heads,                         \n",
    "attn_dropout = attention_dropout,             \n",
    "ff_dropout = ff_dropout,                  \n",
    "mlp_hidden_mults = (4, 2),       \n",
    "cont_embeddings = cont_embeddings,\n",
    "attentiontype = attentiontype,\n",
    "y_dim=y_dim,\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.0001)\n",
    "\n",
    "#scheduler\n",
    "epochs=10\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "\n",
    "best_valid_rmsle = 100000\n",
    "best_valid_ratio = 0\n",
    "RMSLE_best_test_rmsle = 100000\n",
    "ACC_best_test_rmsle = 100000\n",
    "RMSLE_best_test_mae = 100000\n",
    "ACC_best_test_mae = 100000\n",
    "RMSLE_best_test_r2 = -100000\n",
    "ACC_best_test_r2 = -100000\n",
    "RMSLE_best_test_ratio = 0\n",
    "ACC_best_test_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins now.\n",
      "[EPOCH 1] VALID | 0.048 | RMSE: 2.521 / MAE: 66.444 / MSE: 5879.973 / R2: -2.770 / ACC: 0.048\n",
      "[EPOCH 1]  TEST | 0.035 | RMSE: 2.516 / MAE: 66.037 / MSE: 5882.630 / R2: -2.618 / ACC: 0.035\n",
      "[EPOCH 2] VALID | 0.058 | RMSE: 2.345 / MAE: 65.509 / MSE: 5720.528 / R2: -2.668 / ACC: 0.058\n",
      "[EPOCH 2]  TEST | 0.072 | RMSE: 2.343 / MAE: 65.113 / MSE: 5724.020 / R2: -2.521 / ACC: 0.072\n",
      "[EPOCH 3] VALID | 0.088 | RMSE: 2.167 / MAE: 64.300 / MSE: 5518.912 / R2: -2.538 / ACC: 0.088\n",
      "[EPOCH 3]  TEST | 0.090 | RMSE: 2.169 / MAE: 63.919 / MSE: 5523.884 / R2: -2.398 / ACC: 0.090\n",
      "[EPOCH 4] VALID | 0.088 | RMSE: 2.126 / MAE: 63.974 / MSE: 5465.482 / R2: -2.504 / ACC: 0.088\n",
      "[EPOCH 4]  TEST | 0.090 | RMSE: 2.129 / MAE: 63.598 / MSE: 5470.798 / R2: -2.365 / ACC: 0.090\n",
      "[EPOCH 5] VALID | 0.088 | RMSE: 1.928 / MAE: 62.043 / MSE: 5156.163 / R2: -2.306 / ACC: 0.088\n",
      "[EPOCH 5]  TEST | 0.090 | RMSE: 1.937 / MAE: 61.693 / MSE: 5163.832 / R2: -2.176 / ACC: 0.090\n",
      "[EPOCH 6] VALID | 0.100 | RMSE: 1.877 / MAE: 61.430 / MSE: 5058.762 / R2: -2.243 / ACC: 0.100\n",
      "[EPOCH 6]  TEST | 0.107 | RMSE: 1.887 / MAE: 61.089 / MSE: 5067.212 / R2: -2.117 / ACC: 0.107\n",
      "[EPOCH 7] VALID | 0.110 | RMSE: 1.757 / MAE: 59.938 / MSE: 4798.750 / R2: -2.077 / ACC: 0.110\n",
      "[EPOCH 7]  TEST | 0.139 | RMSE: 1.772 / MAE: 59.675 / MSE: 4809.393 / R2: -1.958 / ACC: 0.139\n",
      "[EPOCH 8] VALID | 0.148 | RMSE: 1.626 / MAE: 58.000 / MSE: 4435.082 / R2: -1.843 / ACC: 0.148\n",
      "[EPOCH 8]  TEST | 0.162 | RMSE: 1.647 / MAE: 57.786 / MSE: 4448.972 / R2: -1.736 / ACC: 0.162\n",
      "[EPOCH 9] VALID | 0.148 | RMSE: 1.596 / MAE: 57.462 / MSE: 4337.768 / R2: -1.781 / ACC: 0.148\n",
      "[EPOCH 9]  TEST | 0.162 | RMSE: 1.619 / MAE: 57.261 / MSE: 4352.656 / R2: -1.677 / ACC: 0.162\n",
      "[EPOCH 10] VALID | 0.163 | RMSE: 1.458 / MAE: 54.100 / MSE: 3760.817 / R2: -1.411 / ACC: 0.163\n",
      "[EPOCH 10]  TEST | 0.148 | RMSE: 1.491 / MAE: 54.030 / MSE: 3781.454 / R2: -1.326 / ACC: 0.148\n",
      "TOTAL NUMBER OF PARAMS: 3382342\n",
      "RMSLE-based || MAE:54.03031921386719 | RMSLE:1.4911705255508423 | R2:-1.3258490027971894 | ACC:0.1484375\n",
      "ACC-based || MAE:54.03031921386719 | RMSLE:1.4911705255508423 | R2:-1.3258490027971894 | ACC:0.1484375\n"
     ]
    }
   ],
   "source": [
    "from utils import embed_data, mean_sq_error, count_parameters\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Training begins now.')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            x_categ, x_cont, y_gts, image_feature = data[0].to(device), data[1].to(device),data[2].to(device), data[3].to(device)\n",
    "\n",
    "            # convert the data to embeddings in the next step\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data(x_categ, x_cont, model) \n",
    "\n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc, image_feature)\n",
    "            # select only the representations corresponding to CLS token and apply mlp on it in the next step to get the predictions.\n",
    "            y_reps = reps[:,0,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "\n",
    "            loss = criterion(y_outs,y_gts) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        if epoch%1==0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    valid_mae,valid_mse,valid_rmsle,valid_r2, valid_y_test,valid_y_pred, valid_ratio = mean_sq_error(model, validloader, device)    \n",
    "                    test_mae,test_mse,test_rmsle,test_r2, test_y_test,test_y_pred, test_ratio = mean_sq_error(model, testloader, device)  \n",
    "\n",
    "                    print('[EPOCH %d] VALID | %.3f | RMSE: %.3f / MAE: %.3f / MSE: %.3f / R2: %.3f / ACC: %.3f' %(epoch + 1, valid_ratio, valid_rmsle,valid_mae,valid_mse,valid_r2,valid_ratio ))\n",
    "                    print('[EPOCH %d]  TEST | %.3f | RMSE: %.3f / MAE: %.3f / MSE: %.3f / R2: %.3f / ACC: %.3f' %(epoch + 1, test_ratio, test_rmsle,test_mae,test_mse,test_r2,test_ratio ))\n",
    "                    \n",
    "                    if valid_rmsle < best_valid_rmsle:\n",
    "                        best_valid_rmsle = valid_rmsle\n",
    "                        RMSLE_best_test_rmsle = test_rmsle\n",
    "                        RMSLE_best_test_mae = test_mae\n",
    "                        RMSLE_best_test_r2 = test_r2\n",
    "                        RMSLE_best_test_ratio = test_ratio\n",
    "                        #torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "                    if valid_ratio > best_valid_ratio:\n",
    "                        best_valid_ratio = valid_ratio\n",
    "                        ACC_best_test_rmsle = test_rmsle\n",
    "                        ACC_best_test_mae = test_mae\n",
    "                        ACC_best_test_r2 = test_r2\n",
    "                        ACC_best_test_ratio = test_ratio\n",
    "                model.train()\n",
    "\n",
    "\n",
    "    total_parameters = count_parameters(model)\n",
    "    print('TOTAL NUMBER OF PARAMS: %d' %(total_parameters))\n",
    "    print(f\"RMSLE-based || MAE:{RMSLE_best_test_mae} | RMSLE:{RMSLE_best_test_rmsle} | R2:{RMSLE_best_test_r2} | ACC:{RMSLE_best_test_ratio}\")\n",
    "    print(f\"ACC-based || MAE:{ACC_best_test_mae} | RMSLE:{ACC_best_test_rmsle} | R2:{ACC_best_test_r2} | ACC:{ACC_best_test_ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
